{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "# import argparse\n",
    "from data_generation import generate_data\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import initpath_alg\n",
    "#initpath_alg.init_sys_path()\n",
    "import utilmlab\n",
    "import data_loader_mlab\n",
    "\n",
    "\n",
    "def array2str(a):\n",
    "    s = ''\n",
    "    for idx, el in enumerate(a):\n",
    "        s += (' ' if idx > 0 else '') + '{:0.3f}'.format(el)\n",
    "    return s\n",
    "\n",
    "\n",
    "def one_hot_encoder(a):\n",
    "    n_values = np.max(a) + 1\n",
    "    return np.eye(n_values)[a]\n",
    "\n",
    "\n",
    "def load_create_data(\n",
    "        data_type,\n",
    "        data_out,\n",
    "        is_logging_enabled=True,\n",
    "        fn_csv=None,\n",
    "        label_nm=None):\n",
    "\n",
    "    df_train, df_test, dset = None, None, None\n",
    "    features = None\n",
    "    if data_type in data_loader_mlab.get_available_datasets() + ['show'] \\\n",
    "       or fn_csv is not None:\n",
    "        if fn_csv is not None:\n",
    "            rval, dset = data_loader_mlab.load_dataset_from_csv(\n",
    "                logger, fn_csv, label_nm)\n",
    "        else:\n",
    "            rval, dset = data_loader_mlab.get_dataset(data_type)\n",
    "        assert rval == 0\n",
    "        data_loader_mlab.dataset_log_properties(logger, dset)\n",
    "        if is_logging_enabled:\n",
    "            logger.info('warning no seed')\n",
    "        df = dset['df']\n",
    "        features = dset['features']\n",
    "        labels = dset['targets']\n",
    "        nsample = len(df)\n",
    "        train_ratio = 0.8\n",
    "        idx = np.random.permutation(nsample)\n",
    "        ntrain = int(nsample * train_ratio)\n",
    "        df_train = df.iloc[idx[:ntrain]]\n",
    "        df_test = df.iloc[idx[ntrain:]]\n",
    "\n",
    "        col_drop = utilmlab.col_with_nan(df)\n",
    "        if is_logging_enabled and len(col_drop):\n",
    "            print('warning: dropping features {}'\n",
    "                  ', contains nan'.format(col_drop))\n",
    "            time.sleep(2)\n",
    "\n",
    "        features = [el for el in features if el not in col_drop]\n",
    "\n",
    "        x_train = df_train[features].values\n",
    "        y_train = df_train[labels].values\n",
    "        x_test = df_test[features].values\n",
    "        y_test = df_test[labels].values\n",
    "\n",
    "        g_train, g_test = None, None\n",
    "\n",
    "        y_train = one_hot_encoder(np.ravel(y_train))\n",
    "        y_test = one_hot_encoder(np.ravel(y_test))\n",
    "        if is_logging_enabled:\n",
    "            logger.info('y: train:{} test:{}'.format(\n",
    "                set(np.ravel(y_train)), set(np.ravel(y_test))))\n",
    "    else:\n",
    "        x_train, y_train, g_train = generate_data(\n",
    "            n=train_N, data_type=data_type, seed=train_seed, out=data_out, x_dim = X_DIM)\n",
    "        x_test,  y_test,  g_test = generate_data(\n",
    "            n=test_N,  data_type=data_type, seed=test_seed,  out=data_out, x_dim = X_DIM)\n",
    "    if is_logging_enabled:\n",
    "        logger.info('{} {} {} {}'.format(\n",
    "            x_train.shape,\n",
    "            y_train.shape,\n",
    "            x_test.shape,\n",
    "            y_test.shape))\n",
    "    return x_train, y_train, g_train, x_test, y_test, \\\n",
    "        g_test, df_train, df_test, dset, features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import embed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Generator (Actor) in PyTorch\n",
    "class INVASE_Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(INVASE_Actor, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 100)\n",
    "        self.l2 = nn.Linear(100, 100)\n",
    "        self.l3 = nn.Linear(100, action_dim)\n",
    "\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], 1)\n",
    "        \n",
    "        a = F.selu(self.l1(sa))\n",
    "        a = F.selu(self.l2(a))\n",
    "        return torch.sigmoid(self.l3(a))\n",
    "        \n",
    "# Discriminator (Critic) in PyTorch    \n",
    "# Critic in INVASE is a classifier that provide return signal\n",
    "class INVASE_Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(INVASE_Critic, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 200)\n",
    "        #self.bn1 = nn.BatchNorm1d(num_features=200)\n",
    "        self.l2 = nn.Linear(200, 200)\n",
    "        #self.bn2 = nn.BatchNorm1d(num_features=200)\n",
    "        self.l3 = nn.Linear(200, state_dim)\n",
    "\n",
    "\n",
    "    def forward(self, state, action, mask):\n",
    "        #sa = torch.cat([state, action], 1)\n",
    "        sa = torch.cat([state, mask* action],1)\n",
    "        \n",
    "        #q1 = F.selu(self.bn1(self.l1(sa)))\n",
    "        #q1 = F.selu(self.bn2(self.l2(q1)))\n",
    "        q1 = F.selu(self.l1(sa))\n",
    "        q1 = F.selu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "\n",
    "        return q1 # prob, actually the binary classification result with softmax activation (logits)\n",
    "    \n",
    "# Valuefunction (Baseline) in PyTorch   \n",
    "# Valuefunction in INVASE is a classifier that provide return signal\n",
    "class INVASE_Baseline(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(INVASE_Baseline, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 200)\n",
    "        #self.bn1 = nn.BatchNorm1d(num_features=200)\n",
    "        self.l2 = nn.Linear(200, 200)\n",
    "        #self.bn2 = nn.BatchNorm1d(num_features=200)\n",
    "        self.l3 = nn.Linear(200, state_dim)\n",
    "\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], 1)\n",
    "        #sa = state\n",
    "\n",
    "        q1 = F.selu(self.l1(sa))\n",
    "        q1 = F.selu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "\n",
    "        return q1 # prob, actually the binary classification result with softmax activation (logits)    \n",
    "\n",
    "\n",
    "class PVS():\n",
    "    # 1. Initialization\n",
    "    '''\n",
    "    x_train: training samples\n",
    "    data_type: Syn1 to Syn 6\n",
    "    '''\n",
    "    def __init__(self, xs_train, data_type, nepoch, is_logging_enabled=True, thres = 0.5):\n",
    "        self.is_logging_enabled = is_logging_enabled\n",
    "        self.latent_dim1 = 100      # Dimension of actor (generator) network\n",
    "        self.latent_dim2 = 200      # Dimension of critic (discriminator) network\n",
    "        \n",
    "        self.batch_size = min(1000, xs_train.shape[0])      # Batch size\n",
    "        self.epochs = nepoch        # Epoch size (large epoch is needed due to the policy gradient framework)\n",
    "        self.lamda = 1.0           # Hyper-parameter for the number of selected features \n",
    "        self.thres = thres\n",
    "        '''lamda is number of selected features? is it the coefficient?'''\n",
    "        \n",
    "        \n",
    "        self.input_shape_state = xs_train.shape[1]     # state dimension\n",
    "        self.input_shape_action = xa_train.shape[1]    # action dimension\n",
    "        logger.info('input shape: {}'.format(self.input_shape_state))\n",
    "        \n",
    "        # Actionvation. (For Syn1 and 2, relu, others, selu)\n",
    "        self.activation = 'relu' if data_type in ['Syn1','Syn2'] else 'selu'       \n",
    "        \n",
    "        \n",
    "        self.generator = INVASE_Actor(state_dim=self.input_shape_state, action_dim = self.input_shape_action)\n",
    "        self.discriminator = INVASE_Critic(state_dim=self.input_shape_state, action_dim = self.input_shape_action)\n",
    "        self.valfunction = INVASE_Baseline(state_dim=self.input_shape_state, action_dim = self.input_shape_action)\n",
    "        \n",
    "        \n",
    "        self.generator_optimizer = torch.optim.Adam(self.generator.parameters(), lr=1e-4)#,weight_decay=1e-3)\n",
    "        self.discriminator_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=1e-4)#,weight_decay=1e-3)\n",
    "        self.valfunction_optimizer = torch.optim.Adam(self.valfunction.parameters(), lr=1e-4)#,weight_decay=1e-3)\n",
    "        \n",
    "    def my_loss(self, y_true, y_pred,lmd, Thr):\n",
    "        # dimension of the features\n",
    "        \n",
    "        '''\n",
    "        sel_prob: the mask generated by bernulli sampler [bs, d]\n",
    "        dis_prob: prediction of the critic               [bs, state_dim]\n",
    "        val_prob: prediction of the baseline model       [bs, state_dim]\n",
    "        y_batch: batch of y_train                        [bs, state_dim]\n",
    "        all of those variables are 'placeholders'\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        d = y_pred.shape[1]        \n",
    "        \n",
    "        # Put all three in y_true \n",
    "        # 1. selected probability\n",
    "        sel_prob = y_true[:,:d] # bs x d\n",
    "        # 2. discriminator output\n",
    "        dis_prob = y_true[:,d:(d+self.input_shape_state)] # bs x 2\n",
    "        # 3. valfunction output\n",
    "        val_prob = y_true[:,(d+self.input_shape_state):(d+self.input_shape_state*2)] # bs x 2\n",
    "        # 4. ground truth\n",
    "        y_final = y_true[:,(d+self.input_shape_state*2):] # bs x 2\n",
    "        \n",
    "        # A1. Compute the rewards of the actor network\n",
    "        #embed()\n",
    "        Reward1 = torch.norm(y_final - dis_prob, p=2, dim=1)  \n",
    "\n",
    "        # A2. Compute the rewards of the actor network\n",
    "        Reward2 = torch.norm(y_final - val_prob, p=2, dim=1)  \n",
    "\n",
    "        # Difference is the rewards\n",
    "        Reward =Reward2 -  Reward1\n",
    "\n",
    "        # B. Policy gradient loss computation. \n",
    "        loss1 = Reward * torch.sum(sel_prob * torch.log(y_pred + 1e-8) + (1-sel_prob) * torch.log(1-y_pred + 1e-8), axis = 1) - lmd *torch.mean( torch.abs(y_pred-Thr), axis = 1)\n",
    "        \n",
    "        # C. Maximize the loss1\n",
    "        loss = torch.mean(-loss1)\n",
    "        #embed()\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def Sample_M(self, gen_prob):\n",
    "        # Shape of the selection probability\n",
    "        n = gen_prob.shape[0]\n",
    "        d = gen_prob.shape[1]\n",
    "        # Sampling\n",
    "        samples = np.random.binomial(1, gen_prob, (n,d))\n",
    "\n",
    "        return samples\n",
    "\n",
    "    #%% Training procedure\n",
    "    def train(self, xs_train, xa_train, y_train, lmd, thr):\n",
    "\n",
    "        # For each epoch (actually iterations)\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            #%% Train Discriminator\n",
    "            # Select a random batch of samples\n",
    "            idx = np.random.randint(0, xs_train.shape[0], self.batch_size)\n",
    "            xs_batch = torch.as_tensor(xs_train[idx,:]).float()\n",
    "            xa_batch = torch.as_tensor(xa_train[idx,:]).float()\n",
    "            y_batch = torch.as_tensor(y_train[idx,:]).float() \n",
    "            # y_batch = torch.as_tensor(np.argmax(y_train[idx,:],1)).long()\n",
    "            \n",
    "            # Generate a batch of probabilities of feature selection\n",
    "            gen_prob = self.generator(xs_batch, xa_batch).cpu().detach().numpy()\n",
    "            # Sampling the features based on the generated probability\n",
    "            sel_prob = self.Sample_M(gen_prob)\n",
    "            '''sel_prob is the mask'''\n",
    "            \n",
    "            # Compute the prediction of the critic based on the sampled features (used for generator training)\n",
    "            dis_prob = self.discriminator(xs_batch, xa_batch, torch.as_tensor(sel_prob).float())\n",
    "            \n",
    "            # Train the discriminator\n",
    "            loss_func_c = nn.MSELoss()\n",
    "            self.discriminator_optimizer.zero_grad()\n",
    "            critic_loss = loss_func_c(dis_prob, y_batch)\n",
    "            critic_loss.backward()\n",
    "            self.discriminator_optimizer.step()\n",
    "\n",
    "            #%% Train Valud function\n",
    "\n",
    "            # Compute the prediction of the baseline based on the sampled features (used for generator training)\n",
    "            val_prob = self.valfunction(xs_batch, xa_batch)#.cpu().detach().numpy()\n",
    "            \n",
    "            # Train the baseline model\n",
    "            #v_loss = self.valfunction.train_on_batch(x_batch, y_batch)\n",
    "            loss_func_v = nn.MSELoss()\n",
    "            self.valfunction_optimizer.zero_grad()\n",
    "            value_loss = loss_func_v(val_prob, y_batch)\n",
    "            value_loss.backward()\n",
    "            self.valfunction_optimizer.step()\n",
    "            \n",
    "            \n",
    "            #%% Train Generator\n",
    "            # Use three things as the y_true: sel_prob, dis_prob, and ground truth (y_batch)\n",
    "            '''\n",
    "            sel_prob: the mask generated by bernulli sampler [bs, d]\n",
    "            dis_prob: prediction of the critic               [bs, state_dim]\n",
    "            val_prob: prediction of the baseline model       [bs, state_dim]\n",
    "            y_batch: batch of y_train                        [bs, state_dim]\n",
    "            all of those variables are 'placeholders'\n",
    "            '''\n",
    "            \n",
    "            y_batch_final = torch.as_tensor(np.concatenate( (sel_prob, torch.as_tensor(dis_prob).cpu().detach().numpy(), torch.as_tensor(val_prob).cpu().detach().numpy(), y_train[idx,:]), axis = 1 ))\n",
    "            # Train the generator\n",
    "            \n",
    "            actor_pred = self.generator(xs_batch,xa_batch)\n",
    "            self.generator_optimizer.zero_grad()\n",
    "            actor_loss = self.my_loss(y_batch_final,actor_pred,lmd,Thr)\n",
    "            actor_loss.backward()\n",
    "            self.generator_optimizer.step()\n",
    "            \n",
    "            #%% Plot the progress\n",
    "            dialog = 'Epoch: ' + '{:6d}'.format(epoch) + ', d_loss (Acc)): '\n",
    "            dialog += '{:0.3f}'.format(critic_loss) + ', v_loss (Acc): '\n",
    "            dialog += '{:0.3f}'.format(value_loss) + ', g_loss: ' + '{:+6.4f}'.format(actor_loss)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                logger.info('{}'.format(dialog))\n",
    "    \n",
    "    #%% Selected Features        \n",
    "    def output(self, xs_train, xa_train):\n",
    "        \n",
    "        gen_prob = self.generator(xs_train, xa_train).cpu().detach().numpy()\n",
    "        \n",
    "        return np.asarray(gen_prob)\n",
    "     \n",
    "    #%% Prediction Results \n",
    "    def get_prediction(self, xs, xa, m_train):\n",
    "        \n",
    "        val_prediction = self.valfunction(xs,xa).cpu().detach().numpy()\n",
    "        \n",
    "        dis_prediction = self.discriminator(xs,xa, m_train).cpu().detach().numpy()\n",
    "        \n",
    "        return np.asarray(val_prediction), np.asarray(dis_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " now evaluating: \n",
      "        Pendulum-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\utils\\env_checker.py:200: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1428.961\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1428.961\n",
      "---------------------------------------\n",
      "Total T: 200 Episode Num: 1 Episode T: 200 Reward: -1087.151\n",
      "Total T: 400 Episode Num: 2 Episode T: 200 Reward: -879.068\n",
      "Total T: 600 Episode Num: 3 Episode T: 200 Reward: -1252.688\n",
      "Total T: 800 Episode Num: 4 Episode T: 200 Reward: -1454.375\n",
      "Total T: 1000 Episode Num: 5 Episode T: 200 Reward: -1168.417\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1428.961\n",
      "---------------------------------------\n",
      "recent Evaluation: -1428.9613687610868\n",
      "Total T: 1200 Episode Num: 6 Episode T: 200 Reward: -1426.576\n",
      "Total T: 1400 Episode Num: 7 Episode T: 200 Reward: -1168.346\n",
      "Total T: 1600 Episode Num: 8 Episode T: 200 Reward: -1713.521\n",
      "Total T: 1800 Episode Num: 9 Episode T: 200 Reward: -1343.202\n",
      "Total T: 2000 Episode Num: 10 Episode T: 200 Reward: -976.966\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1428.961\n",
      "---------------------------------------\n",
      "recent Evaluation: -1428.9613687610868\n",
      "Total T: 2200 Episode Num: 11 Episode T: 200 Reward: -1649.565\n",
      "Total T: 2400 Episode Num: 12 Episode T: 200 Reward: -754.511\n",
      "Total T: 2600 Episode Num: 13 Episode T: 200 Reward: -1183.440\n",
      "Total T: 2800 Episode Num: 14 Episode T: 200 Reward: -1505.509\n",
      "Total T: 3000 Episode Num: 15 Episode T: 200 Reward: -952.287\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1428.961\n",
      "---------------------------------------\n",
      "recent Evaluation: -1428.9613687610868\n",
      "Total T: 3200 Episode Num: 16 Episode T: 200 Reward: -780.514\n",
      "Total T: 3400 Episode Num: 17 Episode T: 200 Reward: -1826.907\n",
      "Total T: 3600 Episode Num: 18 Episode T: 200 Reward: -887.671\n",
      "Total T: 3800 Episode Num: 19 Episode T: 200 Reward: -1161.866\n",
      "Total T: 4000 Episode Num: 20 Episode T: 200 Reward: -794.639\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1428.961\n",
      "---------------------------------------\n",
      "recent Evaluation: -1428.9613687610868\n",
      "Total T: 4200 Episode Num: 21 Episode T: 200 Reward: -1176.065\n",
      "Total T: 4400 Episode Num: 22 Episode T: 200 Reward: -1385.023\n",
      "Total T: 4600 Episode Num: 23 Episode T: 200 Reward: -1283.071\n",
      "Total T: 4800 Episode Num: 24 Episode T: 200 Reward: -1648.693\n",
      "Total T: 5000 Episode Num: 25 Episode T: 200 Reward: -1177.145\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1428.961\n",
      "---------------------------------------\n",
      "recent Evaluation: -1428.9613687610868\n",
      "Total T: 5200 Episode Num: 26 Episode T: 200 Reward: -899.195\n",
      "Total T: 5400 Episode Num: 27 Episode T: 200 Reward: -1389.929\n",
      "Total T: 5600 Episode Num: 28 Episode T: 200 Reward: -959.461\n",
      "Total T: 5800 Episode Num: 29 Episode T: 200 Reward: -1631.652\n",
      "Total T: 6000 Episode Num: 30 Episode T: 200 Reward: -981.879\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1428.961\n",
      "---------------------------------------\n",
      "recent Evaluation: -1428.9613687610868\n",
      "Total T: 6200 Episode Num: 31 Episode T: 200 Reward: -1080.138\n",
      "Total T: 6400 Episode Num: 32 Episode T: 200 Reward: -880.952\n",
      "Total T: 6600 Episode Num: 33 Episode T: 200 Reward: -1008.006\n",
      "Total T: 6800 Episode Num: 34 Episode T: 200 Reward: -989.171\n",
      "Total T: 7000 Episode Num: 35 Episode T: 200 Reward: -871.593\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1428.961\n",
      "---------------------------------------\n",
      "recent Evaluation: -1428.9613687610868\n",
      "Total T: 7200 Episode Num: 36 Episode T: 200 Reward: -867.885\n",
      "Total T: 7400 Episode Num: 37 Episode T: 200 Reward: -1601.788\n",
      "Total T: 7600 Episode Num: 38 Episode T: 200 Reward: -1298.748\n",
      "Total T: 7800 Episode Num: 39 Episode T: 200 Reward: -898.191\n",
      "Total T: 8000 Episode Num: 40 Episode T: 200 Reward: -842.501\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1428.961\n",
      "---------------------------------------\n",
      "recent Evaluation: -1428.9613687610868\n",
      "Total T: 8200 Episode Num: 41 Episode T: 200 Reward: -974.276\n",
      "Total T: 8400 Episode Num: 42 Episode T: 200 Reward: -1454.876\n",
      "Total T: 8600 Episode Num: 43 Episode T: 200 Reward: -1336.738\n",
      "Total T: 8800 Episode Num: 44 Episode T: 200 Reward: -1584.393\n",
      "Total T: 9000 Episode Num: 45 Episode T: 200 Reward: -1722.977\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1428.961\n",
      "---------------------------------------\n",
      "recent Evaluation: -1428.9613687610868\n",
      "Total T: 9200 Episode Num: 46 Episode T: 200 Reward: -1395.430\n",
      "Total T: 9400 Episode Num: 47 Episode T: 200 Reward: -1368.733\n",
      "Total T: 9600 Episode Num: 48 Episode T: 200 Reward: -921.689\n",
      "Total T: 9800 Episode Num: 49 Episode T: 200 Reward: -1245.343\n",
      "Total T: 10000 Episode Num: 50 Episode T: 200 Reward: -1103.589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invase: Syn5 300 . Y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1428.961\n",
      "---------------------------------------\n",
      "recent Evaluation: -1428.9613687610868\n",
      "\t\t\t\\t\t\t\t ocsv = <<<<<<<<<<<<<<\n",
      "\t\t\t\t\t\t\t\t repeat = 0\n",
      "\t\t\t\t lkjh.\n",
      "\t\t\t\t\\t\t\ttkszdvnksdjvnlsnbsle\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "(5000, 3) (5000, 101) (5000, 3) (5000, 3) (5000, 101) (5000, 3) (5000, 101)\n",
      "\t\t\t\t\\t\t\ttkszdvnksdjvnlsnbsle\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] The process cannot access the file because it is being used by another process: './log.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d539b6783c4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[0mlabel_nm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;31m# 'target'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[0mnepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m         \u001b[0mlogger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutilmlab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_logger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0modir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Intern_codes\\new\\Action-Refined-Temporal-Difference\\utilmlab.py\u001b[0m in \u001b[0;36minit_logger\u001b[1;34m(odir, log_fn, use_show, log_level)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t\\t\\t\\t\\\\t\\t\\ttkszdvnksdjvnlsnbsle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mhandler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFileHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlog_level\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] The process cannot access the file because it is being used by another process: './log.txt'"
     ]
    }
   ],
   "source": [
    "ENV_NAME = 'Pendulum-v1'\n",
    "alias = 'Fixed_INVASE'\n",
    "RED_ACTION_DIM = 100\n",
    "import gym\n",
    "print('\\n now evaluating: \\n       ', ENV_NAME)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import utils\n",
    "import TD3_INVASE\n",
    "\n",
    "def eval_policy(policy, eval_episodes=10):\n",
    "    eval_env = gym.make(ENV_NAME)\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        state, done = eval_env.reset(), False\n",
    "        while not done:\n",
    "            action = policy.select_action(np.array(state))\n",
    "            state, reward, done, _ = eval_env.step(action[:-RED_ACTION_DIM])\n",
    "            avg_reward += reward\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "    print(\"---------------------------------------\")\n",
    "    return avg_reward\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "#spec = env.action_space\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0] + RED_ACTION_DIM\n",
    "max_action = env.action_space.high[0]\n",
    "\n",
    "args_policy_noise = 0.2\n",
    "args_noise_clip = 0.5\n",
    "args_policy_freq = 2\n",
    "args_max_timesteps = 10000\n",
    "args_expl_noise = 0.1\n",
    "args_batch_size = 256\n",
    "args_eval_freq = 1000\n",
    "args_start_timesteps = 10000\n",
    "\n",
    "kwargs = {\n",
    "    \"state_dim\": state_dim,\n",
    "    \"action_dim\": action_dim,\n",
    "    \"max_action\": max_action,\n",
    "    \"discount\": 0.99,\n",
    "    \"tau\": 0.005\n",
    "}\n",
    "\n",
    "for repeat in range(5):\n",
    "    kwargs[\"policy_noise\"] = args_policy_noise * max_action\n",
    "    kwargs[\"noise_clip\"] = args_noise_clip * max_action\n",
    "    kwargs[\"policy_freq\"] = args_policy_freq\n",
    "    policy = TD3_INVASE.TD3(**kwargs)\n",
    "    replay_buffer = utils.ReplayBuffer(state_dim, action_dim)\n",
    "\n",
    "    # Evaluate untrained policy\n",
    "    evaluations = [eval_policy(policy)]\n",
    "    \n",
    "    state, done = env.reset(), False\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num = 0\n",
    "    counter = 0\n",
    "    msk_list = []        \n",
    "    temp_curve = [eval_policy(policy)]\n",
    "    temp_val = []\n",
    "    for t in range(int(args_max_timesteps)):\n",
    "        episode_timesteps += 1\n",
    "        counter += 1\n",
    "        # Select action randomly or according to policy\n",
    "        if t < args_start_timesteps:\n",
    "            action = np.random.uniform(-max_action, max_action, action_dim)\n",
    "        else:\n",
    "            if np.random.uniform(0,1) < 0.0:\n",
    "                action = np.random.uniform(-max_action, max_action, action_dim)\n",
    "            else:\n",
    "                action = (\n",
    "                    policy.select_action(np.array(state))\n",
    "                    + np.random.normal(0, max_action * args_expl_noise, size=action_dim)\n",
    "                ).clip(-max_action, max_action)\n",
    "\n",
    "        # Perform action\n",
    "        next_state, reward, done, _ = env.step(action[:-RED_ACTION_DIM])\n",
    "        \n",
    "\n",
    "        done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
    "\n",
    "        replay_buffer.add(state, action, next_state, reward, done_bool)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if t >= args_start_timesteps:\n",
    "            '''TD3'''\n",
    "            policy.train(replay_buffer, args_batch_size)\n",
    "                    \n",
    "                    \n",
    "        # Train agent after collecting sufficient data\n",
    "        if done:\n",
    "            print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
    "            msk_list = []\n",
    "            state, done = env.reset(), False\n",
    "            episode_reward = 0\n",
    "            episode_timesteps = 0\n",
    "            episode_num += 1 \n",
    "\n",
    "        # Evaluate episode\n",
    "        if (t + 1) % args_eval_freq == 0:\n",
    "            evaluations.append(eval_policy(policy))\n",
    "            print('recent Evaluation:',evaluations[-1])\n",
    "            np.save('results/evaluations_alias{}_ENV{}_Repeat{}'.format(alias,ENV_NAME,repeat),evaluations)\n",
    "            \n",
    "            \n",
    "    state_list_train = replay_buffer.state[:args_start_timesteps-5000]\n",
    "    state_list_test = replay_buffer.state[args_start_timesteps-5000:args_start_timesteps]\n",
    "\n",
    "    action_list_train = replay_buffer.action[:args_start_timesteps-5000]\n",
    "    action_list_test = replay_buffer.action[args_start_timesteps-5000:args_start_timesteps]\n",
    "    next_state_list_train = replay_buffer.next_state[:args_start_timesteps-5000]\n",
    "    next_state_list_test = replay_buffer.next_state[args_start_timesteps-5000:args_start_timesteps]\n",
    "\n",
    "    state_delta_train = next_state_list_train - state_list_train\n",
    "    state_delta_test = next_state_list_test - state_list_test\n",
    "            \n",
    "            \n",
    "    X_DIM = action_list_train.shape[1] # feature dimension Hyper-Param\n",
    "    \n",
    "    ##################################w\n",
    "    #### what does init_arg do here ??w\n",
    "    ##################################w\n",
    "    \n",
    "    class init_arg(object):\n",
    "        def __init__(self, it = 10000, o = 'feature_score.csv.gz', dataset = None, i= None, target = None):\n",
    "            self.it = it\n",
    "            self.o = o\n",
    "            self.dataset = dataset\n",
    "            self.i = i\n",
    "            self.target = target\n",
    "\n",
    "    args = init_arg(dataset = 'Syn5', it = 300, )\n",
    "    ocsv = args.o # 'feature_score.csv.gz'\n",
    "    # what does os.path.dirname does to odir ?? TODO\n",
    "    odir = os.path.dirname(ocsv)\n",
    "    print('\\t\\t\\t\\\\t\\t\\t\\t ocsv = >>>' + str(odir)+'<<<<<<<<<<<<<<')\n",
    "    odir = '.' if not len(odir) else odir\n",
    "    fn_csv = args.i #'data.csv'\n",
    "    label_nm = args.target # 'target'\n",
    "    nepoch = args.it\n",
    "    \n",
    "    ##############################################################\n",
    "    print('\\t\\t\\t\\t\\t\\t\\t\\t repeat = '+str(repeat))\n",
    "    print('\\t\\t\\t\\t lkjh'+str(odir))\n",
    "    logger = utilmlab.init_logger(odir)\n",
    "    ##############################################################\n",
    "\n",
    "    dataset = args.dataset\n",
    "\n",
    "    assert dataset is not None or fn_csv is not None\n",
    "    assert fn_csv is None or label_nm is not None\n",
    "\n",
    "    # Data output can be either binary (Y) or Probability (Prob)\n",
    "    data_out_sets = ['Y', 'Prob']\n",
    "    data_out = data_out_sets[0]\n",
    "\n",
    "    logger.info('invase: {} {} {} {}'.format(dataset, nepoch, odir, data_out))\n",
    "\n",
    "    # Number of Training and Testing samples\n",
    "    train_N = 10000\n",
    "    test_N = 10000\n",
    "\n",
    "    # Seeds (different seeds for training and testing)\n",
    "    train_seed = 0\n",
    "    test_seed = 1\n",
    "\n",
    "    xs_train,xa_train, y_train, xs_test, xa_test, y_test= state_list_train, action_list_train, state_delta_train, state_list_test, action_list_test, state_delta_test, \n",
    "    g_test = np.zeros((y_test.shape[0],RED_ACTION_DIM + env.action_space.shape[0]))\n",
    "    g_test[:,0] = 1\n",
    "    print(g_test)\n",
    "    print(xs_train.shape, xa_train.shape, y_train.shape, xs_test.shape, xa_test.shape, y_test.shape, g_test.shape)\n",
    "            \n",
    "    '''learning INVASE'''\n",
    "    \n",
    "    REAL_LMD = 1.0 # 0.0 - 0.5\n",
    "\n",
    "\n",
    "    import time\n",
    "    elapsed_time = []\n",
    "\n",
    "    class init_arg(object):\n",
    "        def __init__(self, it = 10000, o = 'feature_score.csv.gz', dataset = None, i= None, target = None):\n",
    "            self.it = it\n",
    "            self.o = o\n",
    "            self.dataset = dataset\n",
    "            self.i = i\n",
    "            self.target = target\n",
    "\n",
    "\n",
    "    for DATASET in ['Syn1']:\n",
    "        args = init_arg(dataset = DATASET, it = 2500,)\n",
    "        ocsv = args.o # 'feature_score.csv.gz'\n",
    "        odir = os.path.dirname(ocsv)\n",
    "        odir = '.' if not len(odir) else odir\n",
    "        fn_csv = args.i #'data.csv'\n",
    "        label_nm = args.target # 'target'\n",
    "        nepoch = args.it\n",
    "        logger = utilmlab.init_logger(odir)\n",
    "\n",
    "        dataset = args.dataset\n",
    "\n",
    "        assert dataset is not None or fn_csv is not None\n",
    "        assert fn_csv is None or label_nm is not None\n",
    "\n",
    "        # Data output can be either binary (Y) or Probability (Prob)\n",
    "        data_out_sets = ['Y', 'Prob']\n",
    "        data_out = data_out_sets[0]\n",
    "\n",
    "        logger.info('invase: {} {} {} {}'.format(dataset, nepoch, odir, data_out))\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "        for thres_i in [0.0]:\n",
    "            Predict_Out_temp = np.zeros([3, 2])    \n",
    "\n",
    "            PVS_Alg = PVS(xs_train, dataset, 100, thres=thres_i)\n",
    "\n",
    "            print('start training......')\n",
    "\n",
    "            for train_epoch in range(int(nepoch/100)):\n",
    "\n",
    "                Lmd = 0.1 #train_epoch*100/nepoch * REAL_LMD\n",
    "                Thr = 0.0 #0.5*(1 - train_epoch*100/nepoch)\n",
    "                print('now at training epoch number', int(train_epoch * 100),'hyp-params: lamda %.4f prior %.4f'%(Lmd,Thr))\n",
    "                PVS_Alg.train(xs_train, xa_train, y_train, lmd = Lmd , thr = Thr)\n",
    "                # 3. Get the selection probability on the testing set\n",
    "                #Sel_Prob_Test = PVS_Alg.output(x_test)\n",
    "\n",
    "\n",
    "\n",
    "                '''recurssive generation'''\n",
    "                input_batch_xs = xs_test * 1.0\n",
    "                input_batch_xa = xa_test * 1.0\n",
    "\n",
    "                sel_prob_tot = 1.0\n",
    "                for recur_time in range(1):\n",
    "                    print('rec time now',recur_time,'dataset now:',DATASET)\n",
    "                    gen_prob = PVS_Alg.generator(torch.as_tensor(input_batch_xs).float(),torch.as_tensor(input_batch_xa).float())\n",
    "                    #sel_prob = PVS_Alg.Sample_M(gen_prob)\n",
    "                    sel_prob = 1.*(gen_prob > 0.5)\n",
    "                    sel_prob_tot_0 = sel_prob_tot * 1.0\n",
    "                    sel_prob_tot = sel_prob * sel_prob_tot\n",
    "                    input_batch_xa = sel_prob_tot * input_batch_xa\n",
    "\n",
    "                    score = sel_prob_tot\n",
    "                    #print('score',score)\n",
    "\n",
    "\n",
    "\n",
    "                    # 4. Selected features\n",
    "                    # 5. Prediction\n",
    "                    val_predict, dis_predict = PVS_Alg.get_prediction(torch.as_tensor(xs_test).float(),torch.as_tensor(xa_test).float(), score)\n",
    "\n",
    "                    def performance_metric(score, g_truth):\n",
    "\n",
    "                        n = len(score)\n",
    "                        Temp_TPR = np.zeros([n,])\n",
    "                        Temp_FDR = np.zeros([n,])\n",
    "\n",
    "                        for i in range(n):\n",
    "\n",
    "                            # TPR    \n",
    "                            # embed()\n",
    "                            TPR_Nom = np.sum((score[i,:] * g_truth[i,:]).cpu().detach().numpy())\n",
    "                            TPR_Den = np.sum(g_truth[i,:])\n",
    "                            Temp_TPR[i] = 100 * float(TPR_Nom)/float(TPR_Den+1e-8)\n",
    "\n",
    "                            # FDR\n",
    "                            FDR_Nom = np.sum((score[i,:] * (1-g_truth[i,:])).cpu().detach().numpy())\n",
    "                            FDR_Den = np.sum(score[i,:].cpu().detach().numpy())\n",
    "                            Temp_FDR[i] = 100 * float(FDR_Nom)/float(FDR_Den+1e-8)\n",
    "\n",
    "                        return np.mean(Temp_TPR), np.mean(Temp_FDR),\\\n",
    "                            np.std(Temp_TPR), np.std(Temp_FDR)\n",
    "\n",
    "                    #%% Output\n",
    "\n",
    "                    TPR_mean, TPR_std = -1, 0\n",
    "                    FDR_mean, FDR_std = -1, 0\n",
    "                    if g_test is not None:\n",
    "                        TPR_mean, FDR_mean, TPR_std, FDR_std = performance_metric(\n",
    "                            score, g_test)\n",
    "\n",
    "                        logger.info('TPR mean: {:0.1f}%  std: {:0.1f}%'.format(\n",
    "                            TPR_mean, TPR_std))\n",
    "                        logger.info('FDR mean: {:0.1f}%  std: {:0.1f}%'.format(\n",
    "                            FDR_mean, FDR_std))\n",
    "                    else:\n",
    "                        logger.info('no ground truth relevance')\n",
    "\n",
    "\n",
    "\n",
    "                    #%% Performance Metrics\n",
    "                    Predict_Out_temp[0,0] = np.linalg.norm(y_test - val_predict,2).mean()\n",
    "                    Predict_Out_temp[0,1] = np.linalg.norm(y_test - dis_predict,2).mean()\n",
    "                    print(Predict_Out_temp)\n",
    "\n",
    "        elapsed_time.append(time.time() - start_time)\n",
    "        print('PyTorch Version: elapsed time for {}: 11 feature, 10000 sample:'.format(DATASET),np.round(elapsed_time,4),'sec.')\n",
    "\n",
    "\n",
    "    '''Continue training with fixed INVASE model'''\n",
    "#     PVS_Alg.generator.cuda()\n",
    "    PVS_Alg.generator.eval()\n",
    "    for t in range(10000, 50000):\n",
    "        episode_timesteps += 1\n",
    "        counter += 1\n",
    "        # Select action randomly or according to policy\n",
    "        if t < args_start_timesteps:\n",
    "            action = np.random.uniform(-max_action, max_action, action_dim)\n",
    "        else:\n",
    "            if np.random.uniform(0,1) < 0.0:\n",
    "                action = np.random.uniform(-max_action, max_action, action_dim)\n",
    "            else:\n",
    "                action = (\n",
    "                    policy.select_action(np.array(state))\n",
    "                    + np.random.normal(0, max_action * args_expl_noise, size=action_dim)\n",
    "                ).clip(-max_action, max_action)\n",
    "\n",
    "        # Perform action\n",
    "        next_state, reward, done, _ = env.step(action[:-RED_ACTION_DIM])\n",
    "\n",
    "\n",
    "        done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
    "\n",
    "        replay_buffer.add(state, action, next_state, reward, done_bool)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if t >= args_start_timesteps:\n",
    "            '''TD3'''\n",
    "            policy.train(replay_buffer, args_batch_size, PVS_Alg)\n",
    "\n",
    "\n",
    "        # Train agent after collecting sufficient data\n",
    "        if done:\n",
    "            print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
    "            msk_list = []\n",
    "            state, done = env.reset(), False\n",
    "            episode_reward = 0\n",
    "            episode_timesteps = 0\n",
    "            episode_num += 1 \n",
    "\n",
    "        # Evaluate episode\n",
    "        if (t + 1) % args_eval_freq == 0:\n",
    "            evaluations.append(eval_policy(policy))\n",
    "            print('recent Evaluation:',evaluations[-1])\n",
    "            np.save('results/evaluations_alias{}_ENV{}_Repeat{}'.format(alias,ENV_NAME,repeat),evaluations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
